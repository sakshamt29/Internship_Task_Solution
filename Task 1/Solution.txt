#Read Employee data from MYSQL Table

df = spark.read \
         .format("jdbc") \
         .option("url", "jdbc:mysql://localhost:3306/assignment1") \
         .option("driver", "com.mysql.jdbc.Driver") \
         .option("dbtable", "employee") \
         .option("user", "root") \
         .option("password", "") \
         .load()
df.show()

_____________________________________
In Terminal
#Set AWS Credentials

export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""

______________________________________

In Pyspark
#Set Hadoop Credentials

sc._jsc.hadoopConfiguration().set('fs.s3a.access.key','')

sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key','')


#Read CSV from S3

df1 =spark.read.format('csv').options(header='true',inferSchema='true').load('s3a://tekionpyspark/employee.csv')
df2 =spark.read.format('csv').options(header='true',inferSchema='true').load('s3a://tekionpyspark/department.csv')


#Union 2 dataframes

resultEmp = df1.union(df2)

#Join resultEmp to dept

resultEmp.join(df2,resultEmp.deptid == df2.id,"inner").show()
